{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN_Tw",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taewonee/srgan_colab/blob/master/SRGAN_Tw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5BG3E2Q5wIY"
      },
      "source": [
        "\n",
        "[Single Image Super-resolution using GAN](https://arxiv.org/abs/1609.04802)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Hv4xZh5vj1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "be2e7f9b-eab0-48cc-9523-0ed02fbf81e2"
      },
      "source": [
        "#tw learning\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn , optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!pip install Pillow==4.0.0\n",
        "!pip install PIL\n",
        "!pip install image\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57e08000 @  0x7fe92d8962a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Collecting image\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ec/51969468a8b87f631cc0e60a6bf1e5f6eec8ef3fd2ee45dc760d5a93b82a/image-1.5.27-py2.py3-none-any.whl\n",
            "Collecting django (from image)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/e5/2676be45ea49cfd09a663f289376b3888accd57ff06c953297bfdee1fb08/Django-2.1.3-py3-none-any.whl (7.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.3MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n",
            "Installing collected packages: django, image\n",
            "Successfully installed django-2.1.3 image-1.5.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iOBahXIopVW",
        "outputId": "1e1ec3f3-1963-4250-b9ce-b81a9af7f0fa"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'srgan_colab'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0aiMZcvvcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "516f8efe-c9ee-4ccf-d529-31a810364698"
      },
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5,  0.5 , 0.5))\n",
        "])\n",
        "\n",
        "trainset = datasets.STL10(root='./data', split='train',download=True, transform=img_transform)\n",
        "\n",
        "testset = datasets.STL10(root='./data', split='test',download=True, transform=img_transform)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggg832Bqwbq7"
      },
      "source": [
        "# code block to text image loader\n",
        "# print(trainloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwY8W455pGhk"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3n2__mWHBQI"
      },
      "source": [
        "CHANNELS = 3\n",
        "HEIGHT = 50\n",
        "WIDTH = 50\n",
        "LR_IMAGE_SIZE = 48\n",
        "EPOCHS = 1\n",
        "LOG_INTERVAL = 500\n",
        "CUDA = torch.cuda.is_available()\n",
        "BATCH_SIZE = 8\n",
        "UPSCALING_FACTOR = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnm3ZKibkSY0"
      },
      "source": [
        "import math\n",
        "irange = range\n",
        "\n",
        "\n",
        "def make_grid(tensor, nrow=8, padding=2,\n",
        "              normalize=False, range=None, scale_each=False, pad_value=0):\n",
        "    \"\"\"Make a grid of images.\n",
        "    Args:\n",
        "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
        "            or a list of images all of the same size.\n",
        "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
        "            The Final grid size is (B / nrow, nrow). Default is 8.\n",
        "        padding (int, optional): amount of padding. Default is 2.\n",
        "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
        "            by subtracting the minimum and dividing by the maximum pixel value.\n",
        "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
        "            then these numbers are used to normalize the image. By default, min and max\n",
        "            are computed from the tensor.\n",
        "        scale_each (bool, optional): If True, scale each image in the batch of\n",
        "            images separately rather than the (min, max) over all images.\n",
        "        pad_value (float, optional): Value for the padded pixels.\n",
        "    Example:\n",
        "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(tensor) or\n",
        "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
        "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
        "\n",
        "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
        "    if isinstance(tensor, list):\n",
        "        tensor = torch.stack(tensor, dim=0)\n",
        "\n",
        "    if tensor.dim() == 2:  # single image H x W\n",
        "        tensor = tensor.view(1, tensor.size(0), tensor.size(1))\n",
        "    if tensor.dim() == 3:  # single image\n",
        "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
        "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
        "        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))\n",
        "\n",
        "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
        "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
        "\n",
        "    if normalize is True:\n",
        "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
        "        if range is not None:\n",
        "            assert isinstance(range, tuple), \\\n",
        "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
        "\n",
        "        def norm_ip(img, min, max):\n",
        "            img.clamp_(min=min, max=max)\n",
        "            img.add_(-min).div_(max - min + 1e-5)\n",
        "\n",
        "        def norm_range(t, range):\n",
        "            if range is not None:\n",
        "                norm_ip(t, range[0], range[1])\n",
        "            else:\n",
        "                norm_ip(t, float(t.min()), float(t.max()))\n",
        "\n",
        "        if scale_each is True:\n",
        "            for t in tensor:  # loop over mini-batch dimension\n",
        "                norm_range(t, range)\n",
        "        else:\n",
        "            norm_range(tensor, range)\n",
        "\n",
        "    if tensor.size(0) == 1:\n",
        "        return tensor.squeeze()\n",
        "\n",
        "    # make the mini-batch of images into a grid\n",
        "    nmaps = tensor.size(0)\n",
        "    xmaps = min(nrow, nmaps)\n",
        "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
        "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
        "    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)\n",
        "    k = 0\n",
        "    for y in irange(ymaps):\n",
        "        for x in irange(xmaps):\n",
        "            if k >= nmaps:\n",
        "                break\n",
        "            grid.narrow(1, y * height + padding, height - padding)\\\n",
        "                .narrow(2, x * width + padding, width - padding)\\\n",
        "                .copy_(tensor[k])\n",
        "            k = k + 1\n",
        "    return grid\n",
        "\n",
        "\n",
        "def save_image(tensor, filename, nrow=8, padding=2,\n",
        "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
        "    \"\"\"Save a given Tensor into an image file.\n",
        "    Args:\n",
        "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
        "            saves the tensor as a grid of images by calling ``make_grid``.\n",
        "        **kwargs: Other arguments are documented in ``make_grid``.\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n",
        "                     normalize=normalize, range=range, scale_each=scale_each)\n",
        "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JkvpCd4iBAq"
      },
      "source": [
        "### Support functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnaWKJ_MiD2S"
      },
      "source": [
        "def sigmoid_mul(x):\n",
        "    return x * F.sigmoid(x)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, cnn, feature_layer=11):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.features = nn.Sequential(*list(cnn.features.children())[:(feature_layer+1)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "\n",
        "class residualBlock(nn.Module):\n",
        "    def __init__(self, in_channels=64, k=3, n=64, s=1):\n",
        "        super(residualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, n, k, stride=s, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n)\n",
        "        self.conv2 = nn.Conv2d(n, n, k, stride=s, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = sigmoid_mul(self.bn1(self.conv1(x)))\n",
        "        return self.bn2(self.conv2(y)) + x\n",
        "\n",
        "class upsampleBlock(nn.Module):\n",
        "    # Implements resize-convolution\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(upsampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
        "        self.shuffler = nn.PixelShuffle(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return sigmoid_mul(self.shuffler(self.conv(x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOVR5jNOU5A7"
      },
      "source": [
        "### Define the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5UBMyfgwO5z"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_residual_blocks, upsample_factor):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_residual_blocks = n_residual_blocks\n",
        "        self.upsample_factor = upsample_factor\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 9, stride=1, padding=4)\n",
        "\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            self.add_module('residual_block' + str(i+1), residualBlock())\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        for i in range(int(self.upsample_factor/2)):\n",
        "            self.add_module('upsample' + str(i+1), upsampleBlock(64, 256))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 3, 9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = sigmoid_mul(self.conv1(x))\n",
        "\n",
        "        y = x.clone()\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            y = self.__getattr__('residual_block' + str(i+1))(y)\n",
        "\n",
        "        x = self.bn2(self.conv2(y)) + x\n",
        "\n",
        "        for i in range(int(self.upsample_factor/2)):\n",
        "            x = self.__getattr__('upsample' + str(i+1))(x)\n",
        "\n",
        "        return self.conv3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJrpjnYFeMm"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Replaced original paper FC layers with FCN\n",
        "        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = sigmoid_mul(self.conv1(x))\n",
        "\n",
        "        x = sigmoid_mul(self.bn2(self.conv2(x)))\n",
        "        x = sigmoid_mul(self.bn3(self.conv3(x)))\n",
        "        x = sigmoid_mul(self.bn4(self.conv4(x)))\n",
        "        x = sigmoid_mul(self.bn5(self.conv5(x)))\n",
        "        x = sigmoid_mul(self.bn6(self.conv6(x)))\n",
        "        x = sigmoid_mul(self.bn7(self.conv7(x)))\n",
        "        x = sigmoid_mul(self.bn8(self.conv8(x)))\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnHrzxRIP0q"
      },
      "source": [
        "def train():\n",
        "  dataloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "  generator = Generator(16, 2)\n",
        "#   print(generator)\n",
        "\n",
        "  discriminator = Discriminator()\n",
        "\n",
        "#   print(discriminator)\n",
        "\n",
        "  # For the content loss\n",
        "  feature_extractor = FeatureExtractor(torchvision.models.vgg19(pretrained=True))\n",
        "#   print(feature_extractor)\n",
        "  content_criterion = nn.MSELoss()\n",
        "  adversarial_criterion = nn.BCELoss()\n",
        "\n",
        "  ones_const = Variable(torch.ones(BATCH_SIZE, 1))\n",
        "\n",
        "  # if gpu is to be used\n",
        "  if CUDA:\n",
        "      generator.cuda()\n",
        "      discriminator.cuda()\n",
        "      feature_extractor.cuda()\n",
        "      content_criterion.cuda()\n",
        "      adversarial_criterion.cuda()\n",
        "      ones_const = ones_const.cuda()\n",
        "\n",
        "  optim_generator = optim.Adam(generator.parameters(), lr=0.0001)\n",
        "  optim_discriminator = optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "  configure('./logs/' + STL10 + '-' + str(BATCH_SIZE) + '-' + str(0.0001) + '-' + str(0.0001), flush_secs=5)\n",
        "  visualizer = Visualizer(image_size=HEIGHT*2)\n",
        "\n",
        "  low_res = torch.FloatTensor(BATCH_SIZE, 3, HEIGHT, WIDTH)\n",
        "\n",
        "  # Pre-train generator using raw MSE loss\n",
        "  print('Generator pre-training')\n",
        "  for epoch in range(2):\n",
        "      mean_generator_content_loss = 0.0\n",
        "\n",
        "      for i, data in enumerate(dataloader):\n",
        "          # Generate data\n",
        "          high_res_real, _ = data\n",
        "\n",
        "          # Downsample images to low resolution\n",
        "          for j in range(BATCH_SIZE):\n",
        "              low_res[j] = scale(high_res_real[j])\n",
        "              high_res_real[j] = normalize(high_res_real[j])\n",
        "\n",
        "          # Generate real and fake inputs\n",
        "          if CUDA:\n",
        "              high_res_real = Variable(high_res_real.cuda())\n",
        "              high_res_fake = generator(Variable(low_res).cuda())\n",
        "          else:\n",
        "              high_res_real = Variable(high_res_real)\n",
        "              high_res_fake = generator(Variable(low_res))\n",
        "\n",
        "          ######### Train generator #########\n",
        "          generator.zero_grad()\n",
        "\n",
        "          generator_content_loss = content_criterion(high_res_fake, high_res_real)\n",
        "          mean_generator_content_loss += generator_content_loss.data[0]\n",
        "\n",
        "          generator_content_loss.backward()\n",
        "          optim_generator.step()\n",
        "\n",
        "          ######### Status and display #########\n",
        "          print('\\r[%d/%d][%d/%d] Generator_MSE_Loss: %.4f' % (epoch, 2, i, len(dataloader), generator_content_loss.data[0]))\n",
        "          visualizer.show(low_res, high_res_real.cpu().data, high_res_fake.cpu().data)\n",
        "\n",
        "      print('\\r[%d/%d][%d/%d] Generator_MSE_Loss: %.4f\\n' % (epoch, 2, i, len(dataloader), mean_generator_content_loss/len(dataloader)))\n",
        "      log_value('generator_mse_loss', mean_generator_content_loss/len(dataloader), epoch)\n",
        "\n",
        "  #  training\n",
        "  optim_generator = optim.Adam(generator.parameters(), lr=0.0001*0.1)\n",
        "  optim_discriminator = optim.Adam(discriminator.parameters(), lr=0.0001*0.1)\n",
        "\n",
        "  print('SRGAN training')\n",
        "  for epoch in range(EPOCHS):\n",
        "      mean_generator_content_loss = 0.0\n",
        "      mean_generator_adversarial_loss = 0.0\n",
        "      mean_generator_total_loss = 0.0\n",
        "      mean_discriminator_loss = 0.0\n",
        "\n",
        "      for i, data in enumerate(dataloader):\n",
        "          # Generate data\n",
        "          high_res_real, _ = data\n",
        "\n",
        "          # Downsample images to low resolution\n",
        "          for j in range(BATCH_SIZE):\n",
        "              low_res[j] = scale(high_res_real[j])\n",
        "              high_res_real[j] = normalize(high_res_real[j])\n",
        "\n",
        "          # Generate real and fake inputs\n",
        "          if CUDA:\n",
        "              high_res_real = Variable(high_res_real.cuda())\n",
        "              high_res_fake = generator(Variable(low_res).cuda())\n",
        "              target_real = Variable(torch.rand(BATCH_SIZE,1)*0.5 + 0.7).cuda()\n",
        "              target_fake = Variable(torch.rand(BATCH_SIZE,1)*0.3).cuda()\n",
        "          else:\n",
        "              high_res_real = Variable(high_res_real)\n",
        "              high_res_fake = generator(Variable(low_res))\n",
        "              target_real = Variable(torch.rand(BATCH_SIZE,1)*0.5 + 0.7)\n",
        "              target_fake = Variable(torch.rand(BATCH_SIZE,1)*0.3)\n",
        "\n",
        "          ######### Train discriminator #########\n",
        "          discriminator.zero_grad()\n",
        "\n",
        "          discriminator_loss = adversarial_criterion(discriminator(high_res_real), target_real) + \\\n",
        "                               adversarial_criterion(discriminator(Variable(high_res_fake.data)), target_fake)\n",
        "          mean_discriminator_loss += discriminator_loss.data[0]\n",
        "\n",
        "          discriminator_loss.backward()\n",
        "          optim_discriminator.step()\n",
        "\n",
        "          ######### Train generator #########\n",
        "          generator.zero_grad()\n",
        "\n",
        "          real_features = Variable(feature_extractor(high_res_real).data)\n",
        "          fake_features = feature_extractor(high_res_fake)\n",
        "\n",
        "          generator_content_loss = content_criterion(high_res_fake, high_res_real) + 0.006*content_criterion(fake_features, real_features)\n",
        "          mean_generator_content_loss += generator_content_loss.data[0]\n",
        "          generator_adversarial_loss = adversarial_criterion(discriminator(high_res_fake), ones_const)\n",
        "          mean_generator_adversarial_loss += generator_adversarial_loss.data[0]\n",
        "\n",
        "          generator_total_loss = generator_content_loss + 1e-3*generator_adversarial_loss\n",
        "          mean_generator_total_loss += generator_total_loss.data[0]\n",
        "\n",
        "          generator_total_loss.backward()\n",
        "          optim_generator.step()   \n",
        "\n",
        "          ######### Status and display #########\n",
        "          print('\\r[%d/%d][%d/%d] Discriminator_Loss: %.4f Generator_Loss (Content/Advers/Total): %.4f/%.4f/%.4f' % (epoch, EPOCHS, i, len(dataloader),\n",
        "          discriminator_loss.data[0], generator_content_loss.data[0], generator_adversarial_loss.data[0], generator_total_loss.data[0]))\n",
        "          visualizer.show(low_res, high_res_real.cpu().data, high_res_fake.cpu().data)\n",
        "\n",
        "      print('\\r[%d/%d][%d/%d] Discriminator_Loss: %.4f Generator_Loss (Content/Advers/Total): %.4f/%.4f/%.4f\\n' % (epoch, EPOCHS, i, len(dataloader),\n",
        "      mean_discriminator_loss/len(dataloader), mean_generator_content_loss/len(dataloader), \n",
        "      mean_generator_adversarial_loss/len(dataloader), mean_generator_total_loss/len(dataloader)))\n",
        "\n",
        "      log_value('generator_content_loss', mean_generator_content_loss/len(dataloader), epoch)\n",
        "      log_value('generator_adversarial_loss', mean_generator_adversarial_loss/len(dataloader), epoch)\n",
        "      log_value('generator_total_loss', mean_generator_total_loss/len(dataloader), epoch)\n",
        "      log_value('discriminator_loss', mean_discriminator_loss/len(dataloader), epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1pyKyVhQp91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5080
        },
        "outputId": "679f6c08-471d-44df-9d24-91ced0c7a5a2"
      },
      "source": [
        "transform = transforms.Compose([transforms.RandomCrop(LR_IMAGE_SIZE*UPSCALING_FACTOR),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
        "                                std = [0.229, 0.224, 0.225])\n",
        "\n",
        "scale = transforms.Compose([transforms.ToPILImage(),\n",
        "                            transforms.Scale(LR_IMAGE_SIZE),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
        "                                                std = [0.229, 0.224, 0.225])\n",
        "                            ])\n",
        "\n",
        "# Equivalent to un-normalizing ImageNet (for correct visualization)\n",
        "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
        "\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "generator = Generator(16, int(UPSCALING_FACTOR))\n",
        "\n",
        "# print(generator)\n",
        "\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# print(discriminator)\n",
        "\n",
        "# For the content loss\n",
        "feature_extractor = FeatureExtractor(torchvision.models.vgg19(pretrained=True))\n",
        "print(feature_extractor)\n",
        "content_criterion = nn.MSELoss()\n",
        "adversarial_criterion = nn.BCELoss()\n",
        "\n",
        "target_real = Variable(torch.ones(BATCH_SIZE,1))\n",
        "target_fake = Variable(torch.zeros(BATCH_SIZE,1))\n",
        "\n",
        "# if gpu is to be used\n",
        "if CUDA:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    feature_extractor.cuda()\n",
        "    content_criterion.cuda()\n",
        "    adversarial_criterion.cuda()\n",
        "    target_real = target_real.cuda()\n",
        "    target_fake = target_fake.cuda()\n",
        "\n",
        "low_res = torch.FloatTensor(BATCH_SIZE, 3, LR_IMAGE_SIZE, LR_IMAGE_SIZE)\n",
        "\n",
        "print('Test started...')\n",
        "mean_generator_content_loss = 0.0\n",
        "mean_generator_adversarial_loss = 0.0\n",
        "mean_generator_total_loss = 0.0\n",
        "mean_discriminator_loss = 0.0\n",
        "\n",
        "# Set evaluation mode (not training)\n",
        "generator.eval()\n",
        "discriminator.eval()\n",
        "\n",
        "for i, data in enumerate(dataloader):\n",
        "    # Generate data\n",
        "    high_res_real, _ = data\n",
        "\n",
        "    # Downsample images to low resolution\n",
        "    for j in range(BATCH_SIZE):\n",
        "        low_res[j] = scale(high_res_real[j])\n",
        "        high_res_real[j] = normalize(high_res_real[j])\n",
        "\n",
        "    # Generate real and fake inputs\n",
        "    if CUDA:\n",
        "        high_res_real = Variable(high_res_real.cuda())\n",
        "        high_res_fake = generator(Variable(low_res).cuda())\n",
        "    else:\n",
        "        high_res_real = Variable(high_res_real)\n",
        "        high_res_fake = generator(Variable(low_res))\n",
        "    \n",
        "    ######### Test discriminator #########\n",
        "\n",
        "    discriminator_loss = adversarial_criterion(discriminator(high_res_real), target_real) + \\\n",
        "                            adversarial_criterion(discriminator(Variable(high_res_fake.data)), target_fake)\n",
        "    mean_discriminator_loss += discriminator_loss.data[0]\n",
        "\n",
        "    ######### Test generator #########\n",
        "\n",
        "    real_features = Variable(feature_extractor(high_res_real).data)\n",
        "    fake_features = feature_extractor(high_res_fake)\n",
        "\n",
        "    generator_content_loss = content_criterion(high_res_fake, high_res_real) + 0.006*content_criterion(fake_features, real_features)\n",
        "    mean_generator_content_loss += generator_content_loss.data[0]\n",
        "    generator_adversarial_loss = adversarial_criterion(discriminator(high_res_fake), target_real)\n",
        "    mean_generator_adversarial_loss += generator_adversarial_loss.data[0]\n",
        "\n",
        "    generator_total_loss = generator_content_loss + 1e-3*generator_adversarial_loss\n",
        "    mean_generator_total_loss += generator_total_loss.data[0]\n",
        "    print(\"i :\"+str(i))\n",
        "    ######### Status and display #########\n",
        "    print('\\r[%d/%d] Discriminator_Loss: %.4f Generator_Loss (Content/Advers/Total): %.4f/%.4f/%.4f' % (i, len(dataloader),\n",
        "    discriminator_loss.data[0], generator_content_loss.data[0], generator_adversarial_loss.data[0], generator_total_loss.data[0]))\n",
        "\n",
        "    for j in range(BATCH_SIZE):\n",
        "        save_image(unnormalize(high_res_real.data[j]), 'output/high_res_real/' + str(i*BATCH_SIZE + j) + '.png')\n",
        "        save_image(unnormalize(high_res_fake.data[j]), 'output/high_res_fake/' + str(i*BATCH_SIZE + j) + '.png')\n",
        "        save_image(unnormalize(low_res[j]), 'output/low_res/' + str(i*BATCH_SIZE + j) + '.png')\n",
        "\n",
        "print (\"out of all loops\")\n",
        "print('\\r[%d/%d] Discriminator_Loss: %.4f Generator_Loss (Content/Advers/Total): %.4f/%.4f/%.4f\\n' % (i, len(dataloader),\n",
        "mean_discriminator_loss/len(dataloader), mean_generator_content_loss/len(dataloader), \n",
        "mean_generator_adversarial_loss/len(dataloader), mean_generator_total_loss/len(dataloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FeatureExtractor(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "  )\n",
            ")\n",
            "Test started...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:93: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:97: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i :0\n",
            "[0/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.8345/0.6900/9.8352\n",
            "i :1\n",
            "[1/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.7037/0.6900/13.7044\n",
            "i :2\n",
            "[2/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9553/0.6900/11.9560\n",
            "i :3\n",
            "[3/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.9347/0.6900/12.9353\n",
            "i :4\n",
            "[4/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.9657/0.6900/13.9664\n",
            "i :5\n",
            "[5/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.5029/0.6900/9.5036\n",
            "i :6\n",
            "[6/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.4157/0.6900/13.4164\n",
            "i :7\n",
            "[7/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.9856/0.6900/10.9863\n",
            "i :8\n",
            "[8/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9293/0.6900/11.9300\n",
            "i :9\n",
            "[9/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.6594/0.6900/13.6601\n",
            "i :10\n",
            "[10/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.6880/0.6900/12.6887\n",
            "i :11\n",
            "[11/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 8.8383/0.6900/8.8390\n",
            "i :12\n",
            "[12/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.4922/0.6900/10.4929\n",
            "i :13\n",
            "[13/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.3580/0.6900/15.3587\n",
            "i :14\n",
            "[14/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.1726/0.6900/14.1733\n",
            "i :15\n",
            "[15/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.3656/0.6900/12.3663\n",
            "i :16\n",
            "[16/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.3407/0.6900/12.3414\n",
            "i :17\n",
            "[17/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.0278/0.6900/11.0285\n",
            "i :18\n",
            "[18/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.3714/0.6900/11.3721\n",
            "i :19\n",
            "[19/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.7513/0.6900/12.7520\n",
            "i :20\n",
            "[20/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.7300/0.6900/11.7307\n",
            "i :21\n",
            "[21/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.7255/0.6900/12.7262\n",
            "i :22\n",
            "[22/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.9346/0.6900/10.9352\n",
            "i :23\n",
            "[23/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.5535/0.6900/15.5542\n",
            "i :24\n",
            "[24/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.1220/0.6900/14.1227\n",
            "i :25\n",
            "[25/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.8887/0.6900/10.8894\n",
            "i :26\n",
            "[26/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.6748/0.6900/10.6755\n",
            "i :27\n",
            "[27/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.6929/0.6900/9.6936\n",
            "i :28\n",
            "[28/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.0038/0.6900/14.0045\n",
            "i :29\n",
            "[29/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.1961/0.6900/12.1968\n",
            "i :30\n",
            "[30/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.6324/0.6900/9.6331\n",
            "i :31\n",
            "[31/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.1818/0.6900/13.1825\n",
            "i :32\n",
            "[32/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9152/0.6900/11.9159\n",
            "i :33\n",
            "[33/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.8514/0.6900/13.8521\n",
            "i :34\n",
            "[34/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.8800/0.6900/9.8807\n",
            "i :35\n",
            "[35/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.2458/0.6900/10.2465\n",
            "i :36\n",
            "[36/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.8421/0.6900/9.8428\n",
            "i :37\n",
            "[37/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.3548/0.6900/13.3555\n",
            "i :38\n",
            "[38/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.2911/0.6900/11.2918\n",
            "i :39\n",
            "[39/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 16.7516/0.6900/16.7523\n",
            "i :40\n",
            "[40/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.0849/0.6900/12.0856\n",
            "i :41\n",
            "[41/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9921/0.6900/11.9928\n",
            "i :42\n",
            "[42/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.4113/0.6900/13.4120\n",
            "i :43\n",
            "[43/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.1789/0.6900/13.1796\n",
            "i :44\n",
            "[44/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.1801/0.6900/12.1808\n",
            "i :45\n",
            "[45/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.9133/0.6900/12.9140\n",
            "i :46\n",
            "[46/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.1627/0.6900/12.1634\n",
            "i :47\n",
            "[47/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.0176/0.6900/14.0183\n",
            "i :48\n",
            "[48/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.1149/0.6900/12.1156\n",
            "i :49\n",
            "[49/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.1316/0.6900/11.1323\n",
            "i :50\n",
            "[50/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.1346/0.6900/11.1353\n",
            "i :51\n",
            "[51/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.7896/0.6900/13.7903\n",
            "i :52\n",
            "[52/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.9896/0.6900/13.9903\n",
            "i :53\n",
            "[53/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.0597/0.6900/15.0603\n",
            "i :54\n",
            "[54/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.2650/0.6900/15.2657\n",
            "i :55\n",
            "[55/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.7006/0.6900/9.7012\n",
            "i :56\n",
            "[56/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.3187/0.6900/11.3194\n",
            "i :57\n",
            "[57/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.4941/0.6900/10.4948\n",
            "i :58\n",
            "[58/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.2125/0.6900/15.2132\n",
            "i :59\n",
            "[59/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.7558/0.6900/11.7565\n",
            "i :60\n",
            "[60/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.0026/0.6900/13.0033\n",
            "i :61\n",
            "[61/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.2140/0.6900/12.2147\n",
            "i :62\n",
            "[62/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.9014/0.6900/14.9021\n",
            "i :63\n",
            "[63/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.8984/0.6900/10.8991\n",
            "i :64\n",
            "[64/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 17.3646/0.6900/17.3653\n",
            "i :65\n",
            "[65/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.5412/0.6900/14.5419\n",
            "i :66\n",
            "[66/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 9.1187/0.6900/9.1194\n",
            "i :67\n",
            "[67/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.5129/0.6900/11.5136\n",
            "i :68\n",
            "[68/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9966/0.6900/11.9973\n",
            "i :69\n",
            "[69/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.0777/0.6900/12.0784\n",
            "i :70\n",
            "[70/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.9116/0.6900/11.9123\n",
            "i :71\n",
            "[71/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.9846/0.6900/15.9853\n",
            "i :72\n",
            "[72/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 16.2486/0.6900/16.2493\n",
            "i :73\n",
            "[73/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 11.3545/0.6900/11.3552\n",
            "i :74\n",
            "[74/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.4044/0.6900/12.4051\n",
            "i :75\n",
            "[75/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.4134/0.6900/13.4141\n",
            "i :76\n",
            "[76/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.0499/0.6900/14.0506\n",
            "i :77\n",
            "[77/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 10.4042/0.6900/10.4049\n",
            "i :78\n",
            "[78/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.5014/0.6900/12.5021\n",
            "i :79\n",
            "[79/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.6665/0.6900/13.6671\n",
            "i :80\n",
            "[80/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.6402/0.6900/13.6409\n",
            "i :81\n",
            "[81/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 14.1055/0.6900/14.1061\n",
            "i :82\n",
            "[82/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 12.6719/0.6900/12.6726\n",
            "i :83\n",
            "[83/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 15.8839/0.6900/15.8846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-10:\n",
            "Process Process-9:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i :84\n",
            "\r[84/1000] Discriminator_Loss: 1.3863 Generator_Loss (Content/Advers/Total): 13.3432/0.6900/13.3439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e0d3da7d8447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_res_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output/high_res_real/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_res_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output/high_res_fake/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output/low_res/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a570c3a8ed49>\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1729\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, check)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     ImageFile._save(im, _idat(fp, chunk),\n\u001b[0;32m--> 787\u001b[0;31m                     [(\"zip\", (0, 0)+im.size, 0, rawmode)])\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IDAT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36mputchunk\u001b[0;34m(fp, cid, *data)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m     \u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mo16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdqFO814nFmd"
      },
      "source": [
        "Reference :https://github.com/eriklindernoren/PyTorch-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Jjq93jYN3C"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmvxejNPYOeD"
      },
      "source": [
        "# torch.cuda.is_available()\n",
        "!ls output/low_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXH3-Rdo2eMz"
      },
      "source": [
        "!rm -rf output/*\n",
        "!mkdir output/low_res\n",
        "!mkdir output/high_res_fake\n",
        "!mkdir output/high_res_real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS6hXFdoMgx6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}